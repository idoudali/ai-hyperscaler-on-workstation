version: "1.0"
metadata:
  name: "hyperscaler-emulation"
  description: "Dual-stack AI infrastructure emulation"

global: {}

clusters:
  hpc:
    name: "hpc-cluster"
    # Default base image path for HPC cluster - can be absolute or relative to the current working directory
    # This provides a fallback if individual nodes don't specify their own images
    # Relative paths will be resolved from the directory where the CLI is executed
    base_image_path: "build/packer/hpc-compute/hpc-compute/hpc-compute.qcow2"
    network:
      subnet: "192.168.100.0/24"
      bridge: "virbr100"

    # Hardware acceleration configuration (x86_64 only)
    hardware:
      acceleration:
        enable_kvm: true           # Enable KVM hardware virtualization
        enable_nested: false      # Enable nested virtualization
        cpu_model: "host-passthrough"  # host, host-passthrough, host-model, qemu64
        cpu_features:              # Additional CPU features
          - "+vmx"                 # Intel VT-x virtualization
          - "+svm"                 # AMD-V virtualization
          - "+sse4.1"              # SSE 4.1 instructions
          - "+sse4.2"              # SSE 4.2 instructions
          - "+avx"                 # AVX instructions (if supported)
          - "+avx2"                # AVX2 instructions (if supported)
        numa_topology: null       # NUMA topology (auto-detect if null)
        cpu_topology:             # CPU topology settings
          sockets: 1
          cores: 4
          threads: 1
        performance:
          enable_hugepages: false  # Use hugepages for better memory performance
          hugepage_size: "2M"      # 2M or 1G hugepages
          cpu_pinning: false       # Pin vCPUs to physical CPUs
          memory_backing: "default" # default, hugepages, memfd
    controller:
      cpu_cores: 4
      memory_gb: 8
      disk_gb: 100
      ip_address: "192.168.100.10"
      # Controller-specific image with SLURM management, database, and monitoring components
      base_image_path: "build/packer/hpc-controller/hpc-controller/hpc-controller.qcow2"

      # VirtIO-FS host directory sharing (optional)
      virtio_fs_mounts:
        # Mount project repository for development
        - tag: "project-repo"
          host_path: "${TOT}"
          mount_point: "${TOT}"
          readonly: false
          owner: "admin"
          group: "admin"
          mode: "0755"
          options: "rw,relatime"

        # Mount datasets (read-only for safety)
        - tag: "datasets"
          host_path: "${HOME}"
          mount_point: "${HOME}"
          readonly: true
          owner: "admin"
          group: "admin"
          mode: "0755"
          options: "ro,relatime"

    compute_nodes:
      - cpu_cores: 8
        memory_gb: 16
        disk_gb: 200
        ip: "192.168.100.11"
        # Compute-specific image with SLURM daemon, container runtime, and GPU support
        base_image_path: "build/packer/hpc-compute/hpc-compute/hpc-compute.qcow2"
        pcie_passthrough:
          enabled: true
          devices:
            # GPU device (primary function) - MUST be listed first for ROM BAR to be enabled correctly
            - pci_address: "0000:01:00.0"
              device_type: "gpu"
              vendor_id: "10de"
              device_id: "2805"
            # Audio device (secondary function) - REQUIRED for same IOMMU group
            - pci_address: "0000:01:00.1"
              device_type: "audio"
              vendor_id: "10de"
              device_id: "22bd"
      - cpu_cores: 8
        memory_gb: 16
        disk_gb: 200
        ip: "192.168.100.12"
        # This node will use the cluster default image (fallback demonstration)
        # base_image_path: "build/packer/hpc-compute/hpc-compute.qcow2"  # Could override here if needed
        pcie_passthrough:
          enabled: true
          devices:
            # GPU device (primary function) - MUST be listed first for ROM BAR to be enabled correctly
            - pci_address: "0000:07:00.0"
              device_type: "gpu"
              vendor_id: "10de"
              device_id: "2504"
            # Audio devices (secondary functions) - REQUIRED for same IOMMU group
            - pci_address: "0000:07:00.1"
              device_type: "audio"
              vendor_id: "10de"
              device_id: "228e"


    slurm_config:
      partitions: ["gpu", "debug"]
      default_partition: "gpu"
      max_job_time: "24:00:00"

    # Storage backend configuration (cluster-wide)
    # See config/README.md for detailed documentation
    storage:
      # BeeGFS parallel filesystem
      beegfs:
        enabled: true  # Enable BeeGFS deployment
        mount_point: "/mnt/beegfs"

        # Service placement (auto-detected from roles if not specified)
        management_node: "controller"  # Management service location
        metadata_nodes:  # Metadata service locations
          - "controller"
        storage_nodes:   # Storage service locations (defaults to all compute nodes)
          - "compute-01"
          - "compute-02"

        # Client configuration
        client_config:
          mount_options: "defaults,_netdev"
          auto_mount: true

      # VirtIO-FS host directory sharing (per-node)
      virtio_fs:
        enabled: true  # Enable VirtIO-FS on applicable nodes

  cloud:
    name: "cloud-cluster"
    # Default base image path for cloud cluster - can be absolute or relative to the current working directory
    # This will be used by all nodes unless they specify their own base_image_path
    # Relative paths will be resolved from the directory where the CLI is executed
    base_image_path: "build/packer/cloud-base/cloud-base/cloud-base.qcow2"
    network:
      subnet: "192.168.200.0/24"
      bridge: "virbr200"
    control_plane:
      cpu_cores: 4
      memory_gb: 8
      disk_gb: 100
      ip_address: "192.168.200.10"
      # Control plane uses cluster default image
      # base_image_path: "build/packer/cloud-controller/cloud-controller.qcow2"  # Could override here

    worker_nodes:
      cpu:
        - worker_type: "cpu"
          cpu_cores: 4
          memory_gb: 8
          disk_gb: 100
          ip: "192.168.200.11"
          # CPU worker uses cluster default image
          # base_image_path: "build/packer/cloud-cpu-worker/cloud-cpu-worker.qcow2"  # Could override here
      gpu:
        - worker_type: "gpu"
          cpu_cores: 8
          memory_gb: 16
          disk_gb: 200
          ip: "192.168.200.12"
          # GPU worker could use specialized image with GPU drivers pre-installed
          # base_image_path: "build/packer/cloud-gpu-worker/cloud-gpu-worker.qcow2"  # Could override here
          pcie_passthrough:
            enabled: true
            devices:
              # GPU device (primary function) - MUST be listed first for ROM BAR to be enabled correctly
              - pci_address: "0000:65:00.2"
                device_type: "gpu"
                vendor_id: "10de"
                device_id: "2684"
                iommu_group: 3
              # Audio device (secondary function) - REQUIRED for same IOMMU group
              - pci_address: "0000:65:00.3"
                device_type: "audio"
                vendor_id: "10de"
                device_id: "22bd"
                iommu_group: 3
        - worker_type: "gpu"
          cpu_cores: 8
          memory_gb: 16
          disk_gb: 200
          ip: "192.168.200.13"
          # This GPU worker uses cluster default (demonstrates fallback)
          pcie_passthrough:
            enabled: true
            devices:
              # GPU device (primary function) - MUST be listed first for ROM BAR to be enabled correctly
              - pci_address: "0000:ca:00.0"
                device_type: "gpu"
                vendor_id: "10de"
                device_id: "1e36"
                iommu_group: 4
              # Audio device (secondary function) - REQUIRED for same IOMMU group
              - pci_address: "0000:ca:00.1"
                device_type: "audio"
                vendor_id: "10de"
                device_id: "22bd"
                iommu_group: 4

    kubernetes_config:
      cni: "calico"
      ingress: "nginx"
      storage_class: "local-path"
