# SLURM GRES Configuration
# Task 023: GPU Resources (GRES) Configuration
# This file defines GPU resources available on compute nodes
# Generated by Ansible - DO NOT EDIT MANUALLY

{% if slurm_gres_autodetect | default(false) | bool %}
# Auto-detection mode using NVML (NVIDIA Management Library)
# This requires NVIDIA drivers and nvidia-smi to be available
AutoDetect=nvml

{% else %}
# Manual GPU configuration
# Format: NodeName=<node> Name=<resource_name> Type=<type> File=<device_file>
{% if slurm_gres_gpu_devices is defined and slurm_gres_gpu_devices | length > 0 %}
{% for gpu in slurm_gres_gpu_devices %}
NodeName={{ slurm_compute_node_name | default(inventory_hostname) }} Name=gpu Type={{ slurm_gres_gpu_type | default('gpu') }} File={{ gpu.device_file }}
{% endfor %}
{% else %}
# No GPU devices configured for this node
# To configure GPUs, set slurm_gres_gpu_devices variable with device information
# Example:
# slurm_gres_gpu_devices:
#   - device_file: /dev/nvidia0
#     type: rtx4090
#   - device_file: /dev/nvidia1
#     type: rtx4090
{% endif %}
{% endif %}

# GRES Resource Limits
# Define resource limits and sharing policies
{% if slurm_gres_shared | default(false) | bool %}
# Shared mode: Multiple jobs can share GPU resources
Shared=yes
{% else %}
# Exclusive mode: GPUs are allocated exclusively to jobs
Shared=no
{% endif %}

# Additional GRES Configuration
{% if slurm_gres_additional_config is defined %}
{{ slurm_gres_additional_config }}
{% endif %}
