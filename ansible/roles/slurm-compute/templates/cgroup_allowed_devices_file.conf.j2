# SLURM Cgroup Allowed Devices Configuration
# Generated by Ansible for HPC SLURM deployment
# Task 024: Set Up Cgroup Resource Isolation
#
# This file specifies which devices jobs are allowed to access
# Used by ConstrainDevices=yes in cgroup.conf
# Reference: https://slurm.schedmd.com/cgroup.conf.html#OPT_AllowedDevicesFile

#=============================================================================
# Device Specification Format
#=============================================================================
#
# Format: /dev/<device>  [type:major:minor access_permissions]
#
# type: 'c' for character device, 'b' for block device
# major:minor: Device numbers (use "ls -l /dev/<device>" to find)
# access_permissions: r (read), w (write), m (mknod)
#
# Wildcard support: '*' matches any major/minor number
#
# Examples:
# /dev/null c 1:3 rwm
# /dev/nvidia* c *:* rwm
# /dev/sda b 8:* r
#

#=============================================================================
# Essential System Devices (Always Allowed)
#=============================================================================

# Standard I/O devices
/dev/null c 1:3 rwm
/dev/zero c 1:5 rwm
/dev/full c 1:7 rwm
/dev/random c 1:8 rwm
/dev/urandom c 1:9 rwm

# Terminal devices
/dev/tty c 5:0 rwm
/dev/pts/* c *:* rwm
/dev/ptmx c 5:2 rwm

# Console devices (for debugging and logging)
/dev/console c 5:1 rwm

#=============================================================================
# Shared Memory and IPC Devices
#=============================================================================

# Shared memory devices (required for MPI and multi-process applications)
/dev/shm/* c *:* rwm

#=============================================================================
# FUSE Devices (Required for Containers)
#=============================================================================

# FUSE device (required for Singularity/Apptainer overlay filesystems)
/dev/fuse c 10:229 rwm

#=============================================================================
# GPU Devices (NVIDIA)
#=============================================================================

{% if slurm_gpu_enabled | default(false) | bool or slurm_cgroup_allow_all_gpus | default(true) | bool %}
# NVIDIA GPU devices
# Note: SLURM automatically grants access to specific GPUs based on GRES allocation
# Jobs without GPU GRES allocation will not have these devices accessible

# NVIDIA device files
/dev/nvidia* c *:* rwm

# NVIDIA unified memory
/dev/nvidia-uvm c *:* rwm
/dev/nvidia-uvm-tools c *:* rwm

# NVIDIA control device
/dev/nvidiactl c *:* rwm

# NVIDIA modeset device (for display management)
/dev/nvidia-modeset c *:* rwm

# NVIDIA capabilities devices (for container GPU support)
/dev/nvidia-caps/* c *:* rwm
{% else %}
# GPU devices disabled (slurm_gpu_enabled: false)
# Jobs will not have access to GPU devices unless explicitly configured
{% endif %}

#=============================================================================
# InfiniBand Devices (Optional - for High-Performance Networking)
#=============================================================================

{% if slurm_infiniband_enabled | default(false) | bool %}
# InfiniBand RDMA devices
/dev/infiniband/* c *:* rwm
/dev/rdma_cm c *:* rwm

# InfiniBand verbs devices
/dev/uverbs* c *:* rwm
{% endif %}

#=============================================================================
# Storage Devices (Typically Restricted)
#=============================================================================

# Block devices are generally NOT allowed for regular jobs
# Uncomment specific devices if needed for specialized workloads

# Example: Allow read-only access to specific disk for data-intensive jobs
{% if slurm_cgroup_allow_block_devices | default(false) | bool %}
# /dev/sda* b 8:* r
# /dev/nvme* b *:* r
{% endif %}

#=============================================================================
# Custom Devices
#=============================================================================

{% if slurm_cgroup_custom_devices is defined %}
# Custom devices defined in inventory/configuration
{% for device in slurm_cgroup_custom_devices %}
{{ device.path }} {{ device.type }} {{ device.major }}:{{ device.minor }} {{ device.perms }}
{% endfor %}
{% endif %}

#=============================================================================
# Device Access Notes and Best Practices
#=============================================================================
#
# Security Considerations:
# 1. Only allow essential devices to minimize attack surface
# 2. GPU devices should be controlled through SLURM GRES allocation
# 3. Block devices (disks) should generally be restricted
# 4. Network devices (if needed) should be carefully controlled
#
# GPU Isolation:
# - SLURM automatically manages GPU device access based on GRES allocation
# - Jobs request GPUs via: srun --gres=gpu:1 ...
# - Only allocated GPUs become accessible to the job
# - This provides hardware-level GPU isolation between jobs
#
# Container Considerations:
# - /dev/fuse is required for Singularity/Apptainer overlay filesystems
# - /dev/shm/* is required for shared memory in multi-process containers
# - /dev/pts/* is required for pseudo-terminal devices in containers
#
# MPI and Distributed Jobs:
# - /dev/shm/* is essential for shared memory MPI implementations
# - InfiniBand devices (if used) require explicit device access
# - Ensure RDMA devices are accessible if using InfiniBand
#
# Troubleshooting:
# - If jobs fail with "Permission denied" for device access, check this file
# - Use "ls -l /dev/<device>" to verify device type and major/minor numbers
# - Test device access: srun --gres=gpu:1 ls -l /dev/nvidia*
# - Check cgroup device controller: cat /sys/fs/cgroup/*/devices/devices.list
#
