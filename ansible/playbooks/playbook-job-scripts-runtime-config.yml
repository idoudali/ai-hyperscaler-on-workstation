---
# SLURM Job Scripts Runtime Configuration Playbook
# Task 025: Runtime configuration playbook for SLURM job scripts (epilog/prolog)
# This playbook is specifically for runtime deployment and testing
# Usage:
#   ansible-playbook -i inventory playbook-job-scripts-runtime-config.yml

- name: SLURM Job Scripts Runtime Configuration
  hosts: "{{ target_hosts | default('all') }}"
  become: true
  gather_facts: true

  vars:
    # Runtime deployment mode (not a Packer build)
    packer_build: false
    hpc_node_type: "compute"

    # SLURM job scripts configuration
    slurm_job_scripts_enabled: true

    # Controller connection (override in inventory or extra-vars)
    slurm_controller_host: "{{ groups['controller'][0] | default('controller') }}"
    slurm_controller_port: 6817

  pre_tasks:
    - name: Verify this is runtime mode
      assert:
        that:
          - not (packer_build | bool)
        fail_msg: "This playbook is for runtime deployment only. packer_build must be false."
        success_msg: "Runtime deployment mode confirmed."

    - name: Check if slurmd is installed
      command: which slurmd
      register: slurmd_check
      changed_when: false
      failed_when: false

    - name: Check if slurm.conf exists
      stat:
        path: /etc/slurm/slurm.conf
      register: slurm_conf_stat

    - name: Display installation status
      debug:
        msg: |
          SLURM Job Scripts Runtime Configuration:
          - Target hosts: {{ target_hosts | default('all') }}
          - Node type: {{ hpc_node_type }}
          - Controller host: {{ slurm_controller_host }}
          - slurmd installed: {{ 'Yes' if slurmd_check.rc == 0 else 'No' }}
          - slurm.conf exists: {{ 'Yes' if slurm_conf_stat.stat.exists else 'No' }}

  roles:
    - role: slurm-compute
      vars:
        packer_build: false
      tags:
        - slurm-job-scripts

  post_tasks:
    - name: Verify job scripts are deployed
      stat:
        path: "{{ item }}"
      register: script_check
      loop:
        - /usr/local/slurm/scripts/epilog.sh
        - /usr/local/slurm/scripts/prolog.sh
        - /usr/local/slurm/tools/diagnose_training_failure.py
      failed_when: not script_check.stat.exists

    - name: Verify epilog script is configured in slurm.conf
      shell: |
        grep -E '^Epilog=' /etc/slurm/slurm.conf || echo "Not configured"
      register: epilog_config
      changed_when: false

    - name: Verify prolog script is configured in slurm.conf
      shell: |
        grep -E '^Prolog=' /etc/slurm/slurm.conf || echo "Not configured"
      register: prolog_config
      changed_when: false

    - name: Display job scripts configuration
      debug:
        msg: |
          Job Scripts Configuration:
          - Epilog: {{ epilog_config.stdout }}
          - Prolog: {{ prolog_config.stdout }}

    - name: Restart slurmd service to apply job scripts configuration
      systemd:
        name: slurmd
        state: restarted
      register: slurmd_restart
      become: true

    - name: Wait for slurmd to be ready after restart
      wait_for:
        timeout: 30
      delegate_to: localhost
      become: false

    - name: Verify slurmd service is running after restart
      systemd:
        name: slurmd
        state: started
      register: slurmd_service_status
      become: true

    - name: Check node registration with controller
      shell: |
        timeout 30 bash -c 'until scontrol show node {{ inventory_hostname }} 2>/dev/null | grep -q "State="; do sleep 2; done'
      register: node_registration
      changed_when: false
      failed_when: false

    - name: Display node state
      command: scontrol show node {{ inventory_hostname }}
      register: node_state
      changed_when: false
      failed_when: false
      when: node_registration.rc == 0

    - name: Test simple SLURM job to trigger epilog/prolog
      shell: |
        srun -N1 hostname
      register: test_job
      changed_when: false
      failed_when: false
      when: node_registration.rc == 0

    - name: Wait for job scripts to execute
      wait_for:
        timeout: 5
      delegate_to: localhost
      when:
        - node_registration.rc == 0
        - test_job.rc == 0

    - name: Check for recent epilog logs
      find:
        paths: /var/log/slurm/epilog
        patterns: "job-*.log"
        age: -2m
        age_stamp: mtime
      register: epilog_logs
      when: node_registration.rc == 0

    - name: Check for recent prolog logs
      find:
        paths: /var/log/slurm/prolog
        patterns: "job-*.log"
        age: -2m
        age_stamp: mtime
      register: prolog_logs
      when: node_registration.rc == 0

    - name: Test failure diagnosis tool
      command: >
        python3 /usr/local/slurm/tools/diagnose_training_failure.py
        --job-id test-validation
        --exit-code 0
      register: diagnosis_test
      changed_when: false
      failed_when: false

    - name: Display runtime configuration results
      debug:
        msg: |
          SLURM Job Scripts Runtime Configuration Complete:
          - Hostname: {{ inventory_hostname }}
          - slurmd service: {{ 'Running' if slurmd_service_status.state == 'started' else 'Failed' }}
          - slurmd restart: {{ 'SUCCESS' if slurmd_restart.changed else 'Already running' }}
          - Node registration: {{ 'SUCCESS' if node_registration.rc == 0 else 'FAILED' }}
          {% if node_registration.rc == 0 %}
          - Node state: {{ node_state.stdout | regex_search('State=\S+') if node_state.rc == 0 else 'Unknown' }}
          - Test job result: {{ 'SUCCESS - ' + test_job.stdout if test_job.rc == 0 else 'FAILED' if test_job.rc is defined else 'Not tested' }}
          - Recent epilog logs: {{ epilog_logs.matched | default(0) }} file(s)
          - Recent prolog logs: {{ prolog_logs.matched | default(0) }} file(s)
          {% endif %}
          - Diagnosis tool test: {{ 'SUCCESS' if diagnosis_test.rc == 0 else 'FAILED' }}

          Job Scripts Status:
          - Epilog script: /usr/local/slurm/scripts/epilog.sh
          - Prolog script: /usr/local/slurm/scripts/prolog.sh
          - Diagnosis tool: /usr/local/slurm/tools/diagnose_training_failure.py
          - Debug logs: /var/log/slurm/job-debug
          - Epilog logs: /var/log/slurm/epilog
          - Prolog logs: /var/log/slurm/prolog

    - name: Display troubleshooting information on failure
      debug:
        msg: |
          Node registration failed. Troubleshooting steps:
          1. Check MUNGE service: systemctl status munge
          2. Verify MUNGE key matches controller
          3. Check network connectivity to controller: {{ slurm_controller_host }}
          4. Review slurmd logs: tail -f /var/log/slurm/slurmd.log
          5. Verify slurm.conf has correct controller hostname
          6. Check firewall rules allowing SLURM traffic
          7. Verify job scripts permissions: ls -la /usr/local/slurm/scripts/
      when: node_registration.rc != 0

    - name: Display job scripts testing information
      debug:
        msg: |
          Job Scripts Testing:
          - Submit a test job to trigger epilog/prolog: srun -N1 hostname
          - Check epilog logs: ls -lt /var/log/slurm/epilog/
          - Check prolog logs: ls -lt /var/log/slurm/prolog/
          - View epilog log: tail -50 /var/log/slurm/epilog/job-*.log
          - View prolog log: tail -50 /var/log/slurm/prolog/job-*.log
          - Test diagnosis tool: python3 /usr/local/slurm/tools/diagnose_training_failure.py --job-id test --exit-code 1
          - Submit a failing job: sbatch --wrap="exit 1"
          - Check debug JSON: ls -lt /var/log/slurm/job-debug/
      when: node_registration.rc == 0

    - name: Fail if node failed to register
      fail:
        msg: |
          SLURM compute node failed to register with controller.
          Please check the troubleshooting steps above.
      when:
        - node_registration.rc != 0
        - fail_on_registration_error | default(false) | bool
