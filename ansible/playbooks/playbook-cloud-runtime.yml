# Cloud Runtime Configuration Playbook
# Unified runtime configuration for complete cloud cluster deployment
# Consolidates Kubespray, ArgoCD, and GitOps deployments into a single maintainable file
#
# Recommended Execution Order:
#   1. Kubespray deployment (tags: kubespray, k8s)
#   2. ArgoCD installation (tags: argocd, gitops)
#   3. GitOps applications (tags: gitops-apps, mlops)
#
# Usage Examples:
#   # Full deployment (all steps)
#   ansible-playbook -i inventories/cloud-cluster/inventory.ini playbooks/playbook-cloud-runtime.yml
#
#   # Deploy only Kubespray
#   ansible-playbook -i inventories/cloud-cluster/inventory.ini playbooks/playbook-cloud-runtime.yml --tags kubespray
#
#   # Deploy ArgoCD only
#   ansible-playbook -i inventories/cloud-cluster/inventory.ini playbooks/playbook-cloud-runtime.yml --tags argocd
#
#   # Deploy GitOps apps only
#   ansible-playbook -i inventories/cloud-cluster/inventory.ini playbooks/playbook-cloud-runtime.yml --tags gitops-apps
#
# IMPORTANT: This playbook is for RUNTIME deployment only (packer_build=false)

# Pre-validation
- name: Cloud Runtime Configuration - Pre-validation
  hosts: localhost
  gather_facts: false
  tags:
    - validation
    - pre-check
    - always
  tasks:
    - name: Verify runtime deployment mode
      ansible.builtin.assert:
        that:
          - not ((packer_build | default(false)) | bool)
        fail_msg: |
          ERROR: This playbook is for RUNTIME deployment only!
          It must be run with packer_build=false on live VMs.
          Do NOT run this during Packer image builds.
        success_msg: "Runtime deployment mode confirmed"

    - name: Display runtime configuration plan
      ansible.builtin.debug:
        msg: |
          ====================================================================
          Cloud Runtime Configuration
          ====================================================================

          This playbook will deploy:
          - Kubernetes cluster via Kubespray
          - Container runtime (containerd)
          - CNI networking (Calico)
          - Core DNS and metrics-server
          - Kubernetes Dashboard
          - ArgoCD for GitOps
          - MinIO and PostgreSQL applications managed through GitOps

          Recommended execution order:
          1. Kubespray deployment (tags: kubespray, k8s)
          2. ArgoCD installation (tags: argocd, gitops)
          3. GitOps applications (tags: gitops-apps, mlops)

          Configuration includes:
          - Kubernetes control plane and worker nodes
          - Container runtime and networking
          - Cluster networking and DNS
          - Kubernetes Dashboard (web UI)
          - ArgoCD for application delivery
          - MinIO and PostgreSQL for MLOps

          ====================================================================

# Pre-flight SSH Key Bootstrap (if needed)
- name: Bootstrap SSH Keys if Needed
  hosts: localhost
  gather_facts: false
  tags:
    - bootstrap
    - ssh-keys
    - preflight
  tasks:
    - name: Test SSH connectivity to all hosts
      ansible.builtin.ping:
      register: ssh_test_result
      changed_when: false
      failed_when: false
      loop: "{{ groups['all'] }}"
      loop_control:
        loop_var: target_host
        label: "{{ target_host }}"
      delegate_to: "{{ target_host }}"
      ignore_errors: true

    - name: Check if any hosts failed SSH
      ansible.builtin.set_fact:
        ssh_failed_hosts: >-
          {{ (
                ssh_test_result.results
                | selectattr('target_host', 'defined')
                | selectattr('failed', 'defined')
                | selectattr('failed')
                | map(attribute='target_host')
                | list
            )
            +
            (
                ssh_test_result.results
                | selectattr('target_host', 'defined')
                | selectattr('unreachable', 'defined')
                | selectattr('unreachable')
                | map(attribute='target_host')
                | list
            )
          | unique | list }}
      delegate_to: localhost

    - name: Display bootstrap requirement
      ansible.builtin.debug:
        msg: "SSH bootstrap required for hosts: {{ ssh_failed_hosts }}"
      when: ssh_failed_hosts | length > 0
      delegate_to: localhost

    - name: Pause to allow manual SSH key bootstrap if needed
      ansible.builtin.pause:
        seconds: 30
      when: ssh_failed_hosts | length > 0
      delegate_to: localhost

    - name: Display SSH key bootstrap instructions
      ansible.builtin.debug:
        msg: |
          One or more hosts failed SSH connectivity.
          Please ensure SSH keys are bootstrapped for these hosts: {{ ssh_failed_hosts }}
          If you have just performed manual SSH key setup, wait for 30 seconds before retrying.
      when: ssh_failed_hosts | length > 0
      delegate_to: localhost

# Pre-flight - Prepare Debian nodes
- name: Pre-flight - Prepare Debian nodes
  hosts: all
  gather_facts: true
  become: true
  tags:
    - preflight
    - setup
    - kubespray
  tasks:
    - name: Prefer IPv4 connectivity (disable IPv6)
      ansible.builtin.include_role:
        name: network-ipv6
      vars:
        disable_ipv6: true

    - name: Update APT cache on Debian systems
      ansible.builtin.apt:
        update_cache: true
        cache_valid_time: 3600
      when: ansible_os_family == 'Debian'
      register: apt_update
      retries: 3
      delay: 5
      until: apt_update is succeeded

    - name: Pre-install conntrack package (Debian 13+)
      ansible.builtin.apt:
        name: conntrack
        state: present
      when:
        - ansible_os_family == 'Debian'
        - ansible_distribution_major_version | int >= 13
      register: conntrack_install
      retries: 3
      delay: 5
      until: conntrack_install is succeeded

    - name: Ensure DNS configuration present
      ansible.builtin.include_role:
        name: dns-config

    - name: Debug OS family
      ansible.builtin.debug:
        msg: "OS family: {{ ansible_os_family | default('NOT SET') }}, Distribution: {{ ansible_distribution | default('NOT SET') }}"

    - name: Fix APT permissions for Kubespray (fix extended_states)
      ansible.builtin.debug:
        msg: "Fixing APT extended_states permissions for {{ ansible_os_family }}"
      when: ansible_os_family == 'Debian'

    - name: Ensure /var/lib/apt directory exists and has correct permissions
      ansible.builtin.file:
        path: /var/lib/apt
        state: directory
        owner: root
        group: root
        mode: '0755'
      when: ansible_os_family == 'Debian'

    - name: Fix ownership and permissions of /var/lib/apt directory recursively
      ansible.builtin.shell: chown -R root:root /var/lib/apt && chmod -R u+rwX,go+rX /var/lib/apt
      changed_when: false
      when: ansible_os_family == 'Debian'

    - name: Ensure all subdirectories under /var/lib/apt are writable
      ansible.builtin.command:
        cmd: find /var/lib/apt -type d -exec chmod 755 {} \;
      changed_when: false
      when: ansible_os_family == 'Debian'

    - name: Ensure all files under /var/lib/apt have correct permissions
      ansible.builtin.command:
        cmd: find /var/lib/apt -type f -exec chmod 644 {} \;
      changed_when: false
      when: ansible_os_family == 'Debian'

    - name: Remove /var/lib/apt/extended_states if it exists to recreate with proper permissions
      ansible.builtin.file:
        path: /var/lib/apt/extended_states
        state: absent
      when: ansible_os_family == 'Debian'

    - name: Create /var/lib/apt/extended_states with correct permissions
      ansible.builtin.file:
        path: /var/lib/apt/extended_states
        state: touch
        owner: root
        group: root
        mode: '0644'
      when: ansible_os_family == 'Debian'

    - name: Verify /var/lib/apt/extended_states permissions after fix
      ansible.builtin.stat:
        path: /var/lib/apt/extended_states
      register: apt_extended_states_verify
      when: ansible_os_family == 'Debian'

    - name: Display APT extended_states permissions
      ansible.builtin.debug:
        msg: "APT extended_states: exists={{ apt_extended_states_verify.stat.exists | default(false) }}, owner={{ apt_extended_states_verify.stat.uid | default('unknown') }}, group={{ apt_extended_states_verify.stat.gid | default('unknown') }}, mode={{ apt_extended_states_verify.stat.mode | default('unknown') }}"
      when:
        - ansible_os_family == 'Debian'
        - apt_extended_states_verify.stat is defined

    - name: Test apt-mark can write to extended_states (verify fix worked)
      ansible.builtin.shell: apt-mark showmanual | head -1
      register: apt_mark_test
      changed_when: false
      failed_when: false
      when: ansible_os_family == 'Debian'

    - name: Display apt-mark test result
      ansible.builtin.debug:
        msg: "apt-mark test: rc={{ apt_mark_test.rc | default('unknown') }}, stdout={{ apt_mark_test.stdout | default('') }}"
      when:
        - ansible_os_family == 'Debian'
        - apt_mark_test is defined

    - name: Disable APT extended_states to avoid permission issues (optional safety measure)
      ansible.builtin.lineinfile:
        path: /etc/apt/apt.conf.d/99-disable-extended-states
        line: 'APT::Extended-States "0";'
        create: true
        owner: root
        group: root
        mode: '0644'
      when: ansible_os_family == 'Debian'

# Deploy Kubernetes with Kubespray
- name: Deploy Cloud Cluster with Kubespray
  hosts: localhost
  gather_facts: false
  become: false
  tags:
    - kubespray
    - k8s
    - kubernetes
  vars:
    kubespray_source_dir: "{{ playbook_dir }}/../../build/3rd-party/kubespray/kubespray-src"
    kubespray_inventory_file: "{{ inventory_file | default('ansible/inventories/cloud-cluster/inventory.ini') }}"
  tasks:
    - name: Display deployment information
      ansible.builtin.debug:
        msg: |
          ================================================================
          Cloud Cluster Kubernetes Deployment
          ================================================================
          Inventory: {{ kubespray_inventory_file }}
          Kubespray: {{ kubespray_source_dir }}
          ================================================================
      delegate_to: localhost
      run_once: true

    - name: Run Kubespray cluster deployment via role
      ansible.builtin.include_role:
        name: kubespray-integration

# Post-Kubespray Configuration
- name: Post-Kubespray Configuration
  hosts: kube_node
  gather_facts: true
  become: true
  tags:
    - kubespray
    - post-kubespray
  tasks:
  # Note: Kubespray already ensures nodes are Ready during deployment
  # Skipping redundant node readiness check to avoid Python dependency issues

# Validate Cluster Health
- name: Validate Cluster Health
  hosts: kube_control_plane[0]
  gather_facts: false
  become: false
  tags:
    - kubespray
    - validation
    - k8s-validation
  tasks:
    - name: Get control plane host address
      ansible.builtin.set_fact:
        control_plane_host: "{{ hostvars[groups['kube_control_plane'][0]]['ansible_host'] | default(hostvars[groups['kube_control_plane'][0]]['inventory_hostname']) }}"
      run_once: true

    - name: Wait for API server to be ready
      ansible.builtin.wait_for:
        host: "{{ control_plane_host }}"
        port: 6443
        delay: 10
        timeout: 300
      delegate_to: localhost
      run_once: true

    - name: Display deployment success message
      ansible.builtin.debug:
        msg: |
          ================================================================
          Kubernetes Cluster Deployment Complete
          ================================================================
          API Server: {{ control_plane_host }}:6443
          Status: Ready

          Note: Use 'kubectl get nodes' to verify cluster status.
          Kubespray has already validated all nodes are Ready.
          ================================================================
      run_once: true

# Copy kubeconfig from control plane to localhost
- name: Copy kubeconfig to localhost
  hosts: kube_control_plane[0]
  gather_facts: false
  become: false
  tags:
    - kubespray
    - kubeconfig
    - argocd
  vars:
    # Extract cluster name from inventory path or use default
    # Default to 'cloud-cluster' if not specified
    cluster_name: "{{ kubeconfig_cluster_name | default('cloud-cluster') }}"
    # Save kubeconfig in repo output directory (gitignored)
    kubeconfig_dir: "{{ playbook_dir }}/../../output/cluster-state/kubeconfigs"
    kubeconfig_filename: "{{ cluster_name }}.kubeconfig"
    kubeconfig_path: "{{ kubeconfig_dir }}/{{ kubeconfig_filename }}"
  tasks:
    - name: Get control plane host address
      ansible.builtin.set_fact:
        control_plane_host: "{{ hostvars[groups['kube_control_plane'][0]]['ansible_host'] | default(hostvars[groups['kube_control_plane'][0]]['inventory_hostname']) }}"
      run_once: true

    - name: Find kubeconfig location on control plane
      ansible.builtin.stat:
        path: "{{ item }}"
      register: kubeconfig_check
      loop:
        - /etc/kubernetes/admin.conf
        - ~/.kube/config
        - ~admin/.kube/config
      failed_when: false
      changed_when: false

    - name: Get source kubeconfig path
      ansible.builtin.set_fact:
        source_kubeconfig_path: "{{ (kubeconfig_check.results | selectattr('stat.exists', 'equalto', true) | map(attribute='item') | first) | default('/etc/kubernetes/admin.conf') }}"
      run_once: true

    - name: Ensure kubeconfig output directory exists
      ansible.builtin.file:
        path: "{{ kubeconfig_dir }}"
        state: directory
        mode: '0755'
      delegate_to: localhost
      run_once: true
      become: false

    - name: Copy kubeconfig to readable temp location on control plane
      ansible.builtin.copy:
        src: "{{ source_kubeconfig_path }}"
        dest: /tmp/admin.conf.tmp
        remote_src: true
        mode: '0644'
      run_once: true
      become: true
      # This become applies to control plane (not localhost) since task runs on control plane

    - name: Read kubeconfig content from control plane
      ansible.builtin.slurp:
        src: /tmp/admin.conf.tmp
      register: kubeconfig_content
      run_once: true
      become: false
      # No sudo needed - file is readable after copy

    - name: Write kubeconfig to localhost
      ansible.builtin.copy:
        content: "{{ kubeconfig_content.content | b64decode }}"
        dest: "{{ kubeconfig_path }}"
        mode: '0644'
      delegate_to: localhost
      run_once: true
      become: false

    - name: Clean up temp kubeconfig on control plane
      ansible.builtin.file:
        path: /tmp/admin.conf.tmp
        state: absent
      run_once: true
      become: false
      failed_when: false

    - name: Update kubeconfig server URL to use control plane IP
      ansible.builtin.replace:
        path: "{{ kubeconfig_path }}"
        regexp: 'server: https://127\.0\.0\.1:\d+'
        replace: "server: https://{{ control_plane_host }}:6443"
      delegate_to: localhost
      run_once: true
      become: false
      when: control_plane_host is defined

    - name: Update kubeconfig server URL (alternative format)
      ansible.builtin.replace:
        path: "{{ kubeconfig_path }}"
        regexp: 'server: https://localhost:\d+'
        replace: "server: https://{{ control_plane_host }}:6443"
      delegate_to: localhost
      run_once: true
      become: false
      when: control_plane_host is defined

    - name: Update context name in kubeconfig to include cluster name
      ansible.builtin.replace:
        path: "{{ kubeconfig_path }}"
        regexp: 'name: kubernetes-admin@kubernetes'
        replace: "name: kubernetes-admin@{{ cluster_name }}"
      delegate_to: localhost
      run_once: true
      become: false

    - name: Update cluster name in kubeconfig
      ansible.builtin.replace:
        path: "{{ kubeconfig_path }}"
        regexp: '    cluster: kubernetes'
        replace: "    cluster: {{ cluster_name }}"
      delegate_to: localhost
      run_once: true
      become: false

    - name: Display kubeconfig location and usage instructions
      ansible.builtin.debug:
        msg: |
          ================================================================
          Kubeconfig Saved for Cluster: {{ cluster_name }}
          ================================================================
          Location: {{ kubeconfig_path }}
          Kubernetes API Server: {{ control_plane_host }}:6443

          To use this cluster:
            export KUBECONFIG={{ kubeconfig_path }}
            kubectl get nodes

          Or use absolute path from project root:
            export KUBECONFIG=$(pwd)/output/cluster-state/kubeconfigs/{{ kubeconfig_filename }}
            kubectl get nodes

          To manage multiple clusters, use:
            scripts/manage-kubeconfig.sh use {{ cluster_name }}

          All kubeconfigs are stored in: output/cluster-state/kubeconfigs/
          (This directory is gitignored for security)
          ================================================================
      delegate_to: localhost
      run_once: true
      become: false

# Deploy Kubernetes Dashboard
- name: Deploy Kubernetes Dashboard
  hosts: localhost
  gather_facts: false
  vars:
    cluster_name: "{{ kubeconfig_cluster_name | default('cloud-cluster') }}"
    kubeconfig_path: "{{ playbook_dir }}/../../output/cluster-state/kubeconfigs/{{ cluster_name }}.kubeconfig"
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
  tags:
    - dashboard
    - k8s-dashboard
  roles:
    - role: k8s-dashboard

# Validate ArgoCD Prerequisites
- name: Validate ArgoCD Prerequisites
  hosts: localhost
  gather_facts: false
  vars:
    cluster_name: "{{ kubeconfig_cluster_name | default('cloud-cluster') }}"
    kubeconfig_dir: "{{ playbook_dir }}/../../output/cluster-state/kubeconfigs"
    kubeconfig_path: "{{ kubeconfig_dir }}/{{ cluster_name }}.kubeconfig"
  tags:
    - argocd
    - validation
    - pre-argocd
  tasks:
    - name: Check kubectl is available
      ansible.builtin.command: kubectl version --client -o json
      register: kubectl_version
      changed_when: false
      failed_when: kubectl_version.rc != 0

    - name: Check Kubernetes cluster is accessible
      ansible.builtin.command: kubectl cluster-info
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"
      register: cluster_info
      changed_when: false
      failed_when: cluster_info.rc != 0

    - name: Display cluster information
      ansible.builtin.debug:
        msg: "{{ cluster_info.stdout_lines }}"

    - name: Check kubernetes.core collection
      ansible.builtin.command: ansible-galaxy collection list kubernetes.core
      register: k8s_collection
      changed_when: false
      failed_when: k8s_collection.rc != 0

    - name: Check if kubernetes library is already installed
      ansible.builtin.command: "{{ ansible_python_interpreter | default('/usr/bin/python3') }} -c 'import kubernetes; print(kubernetes.__version__)'"
      register: k8s_lib_check
      changed_when: false
      failed_when: false
      delegate_to: localhost
      run_once: true

    - name: Fail if kubernetes library is not installed
      ansible.builtin.fail:
        msg: |
          kubernetes Python library is not installed.
          Please run: make venv-create
          This will install kubernetes along with other Ansible dependencies.
      when: k8s_lib_check.rc != 0
      delegate_to: localhost
      run_once: true

    - name: Display prerequisites check
      ansible.builtin.debug:
        msg:
          - "✓ kubectl is available"
          - "✓ Kubernetes cluster is accessible"
          - "✓ kubernetes.core collection is installed"
          - "✓ kubernetes Python library is installed"
          - "Ready to deploy ArgoCD"

# Deploy ArgoCD
- name: Deploy ArgoCD
  hosts: localhost
  gather_facts: false
  vars:
    cluster_name: "{{ kubeconfig_cluster_name | default('cloud-cluster') }}"
    kubeconfig_dir: "{{ playbook_dir }}/../../output/cluster-state/kubeconfigs"
    kubeconfig_path: "{{ kubeconfig_dir }}/{{ cluster_name }}.kubeconfig"
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
  tags:
    - argocd
    - gitops
  roles:
    - role: argocd

# Display ArgoCD Next Steps
- name: Display ArgoCD Next Steps
  hosts: localhost
  gather_facts: false
  tags:
    - argocd
    - info
  tasks:
    - name: Display GitOps setup instructions
      ansible.builtin.debug:
        msg:
          - "================================================================"
          - "ArgoCD Deployed Successfully"
          - "================================================================"
          - ""
          - "Next Steps:"
          - ""
          - "1. Access ArgoCD UI:"
          - "   kubectl port-forward svc/argocd-server -n argocd 8080:443"
          - "   URL: https://localhost:8080 (accept the self-signed certificate)"
          - ""
          - "2. Login with credentials displayed above"
          - ""
          - "3. Configure Git repositories for your applications:"
          - "   Edit k8s-manifests/argocd-apps/*.yaml to point at your Git remote"
          - "   Commit and push changes so ArgoCD can track them"
          - ""
          - "4. Define applications in Git:"
          - "   Store Kubernetes manifests under k8s-manifests/"
          - "   (See docs/workflows/gitops-deployment-workflow.md for structure and examples)"
          - ""
          - "5. Trigger syncs via ArgoCD UI or CLI once manifests are committed"
          - ""
          - "Refer to docs/workflows/gitops-deployment-workflow.md for the full GitOps process."
          - ""
          - "================================================================"

# Post-validation
- name: Cloud Runtime Configuration - Post-validation
  hosts: localhost
  gather_facts: false
  tags:
    - validation
    - post-check
    - summary
  tasks:
    - name: Display configuration completion
      ansible.builtin.debug:
        msg: |
          ====================================================================
          Cloud Runtime Configuration Complete
          ====================================================================

          Cluster configured successfully!

          Deployed Components:
          1. Kubernetes Cluster (via Kubespray)
             - Control plane nodes
             - Worker nodes
             - Container runtime (containerd)
             - CNI networking (Calico)
             - CoreDNS
             - Metrics-server
          2. Kubernetes Dashboard
             - Web UI for cluster management
             - Access via port-forward (see Dashboard deployment output)

          3. ArgoCD (GitOps control plane)
             - ArgoCD server and controllers
             - UI available for GitOps application management

          Access Points:
          - Kubernetes API: Use kubectl with generated kubeconfig
          - Kubeconfig location: See output above
          - Dashboard UI: See Dashboard deployment output above
          - ArgoCD UI: kubectl port-forward svc/argocd-server -n argocd 8080:443 (https://localhost:8080)

          Next Steps:
          1. Verify cluster status: kubectl get nodes
          2. Access ArgoCD UI and login using the credentials above
          3. Update ArgoCD application manifests in k8s-manifests/ to point to your Git repository
          4. Review docs/workflows/gitops-deployment-workflow.md for managing applications via GitOps
          5. Configure kubectl: See Kubespray output for kubeconfig location

          For troubleshooting:
          - Kubespray logs: Check Ansible output above
          - ArgoCD logs: kubectl logs -n argocd -l app.kubernetes.io/name=argocd-server
          - Application status: argocd app list

          ====================================================================
